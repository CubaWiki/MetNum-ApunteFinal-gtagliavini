\section{Interpolación polinómica}

\subsection{Problema}

Al igual que antes, supongamos que tenemos una muestra de valores $(x_0, y_0), \cdots, (x_n, y_n)$. Queremos encontrar un polinomio $P$ que interpole dichos puntos, es decir, que cumpla $P(x_i) = y_i$ para cada $i = 0, \cdots, n$.

\subsection{Polinomio interpolador de Lagrange}

\begin{teo}
Consideremos $n + 1$ puntos $(x_0, y_0), \cdots, (x_n, y_n) \in \mathbb{R}^2$ con $x_i \neq x_j$ si $i \neq j$. Entonces existe un único polinomio $P \in \mathbb{R}[x]$ de grado menor o igual que $n$ tal que $P(x_i) = y_i$ para todo $i = 0, \cdots, n$.

\begin{proof}
Probemos primero la existencia. Definimos

\[L_{n,k}(x) = \frac{(x - x_0) \cdots (x - x_{k - 1})(x - x_{k + 1}) \cdots (x - x_n)}{(x_k - x_0)\cdots (x_k - x_{k - 1})(x_k - x_{k + 1})\cdots (x_k - x_n)} = \prod_{i \neq k}^n \frac{x - x_i}{x_k - x_i}\]

Este polinomio es de grado $n$ y cumple

\[
L_{n,k}(x_i)
\left\{
	\begin{array}{ll}
		1  & \mbox{si } i = k \\
		0 & \mbox{si } i \neq k
	\end{array}
\right.
\]

Definimos

\[P(x) = \sum_{k = 0}^n y_k L_{n, k}(x)\]

Tenemos que $\text{deg}(P) = \text{deg}(\sum_{k = 0}^n y_k L_{n, k}) \leq \max \limits_{0 \leq k \leq n} \text{deg}(y_k L_{n, k}) \leq n$. Además

\[P(x_i) = \sum_{k = 0}^n y_k L_{n, k}(x_i) = y_i L_{n,i}(x_i) = y_i\]

con lo cual $P \in \mathbb{R}[X]$ es un polinomio que cumple lo deseado.

Veamos la unicidad. Por el absurdo, supongamos que existe un polinomio $Q \in \mathbb{R}_n[X] - \{P\}$ que interpola los $n + 1$ puntos. Sea $R = P - Q \neq 0$, que es tal que $\text{deg}(R) \leq \max\{\text{deg}(P), \text{deg}(Q)\} \leq n$, con lo cual tiene a lo sumo $n$ raíces distintas en $\mathbb{R}$. Sin embargo, para cada $i = 0, \cdots, n$, $R(x_i) = P(x_i) - Q(x_i) = 0$, que son $n + 1$ valores distintos, lo cual es una contradicción.
\end{proof}
\end{teo}

Este polinomio se conoce con el nombre de \textit{polinomio interpolador de Lagrange}. En particular, este polinomio se puede utilizar para aproximar una función $f(x)$ interpolando puntos $(x_0, f(x_0)), \cdots, (x_n, f(x_n))$. Por este motivo, interesa conocer una expresión de la forma

\[f(x) = P(x) + R(x)\]

donde $R(x)$ es el error de la aproximación.

\begin{propo}
Sean $x_0, \cdots, x_n \in [a, b]$ distintos. Sea $f \in C^{n + 1}([a, b])$. Sea $P$ el polinomio interpolador de Lagrange en $(x_0, f(x_0)), \cdots, (x_n, f(x_n))$. Entonces para todo $x \in [a, b]$ existe $\xi(x) \in (a, b)$ tal que

\[f(x) = P(x) + \frac{f^{n + 1}(\xi(x))}{(n + 1)!} (x - x_0) \cdots (x - x_n)\]
\end{propo}

Por claridad, cuando tratemos con puntos $x_0, \cdots, x_n$, llamaremos $P_{m_1, \cdots, m_k}$ ($m_i \neq m_j$ si $i \neq j$ y $m_i \in \{0, \cdots, n\}$) al polinomio interpolador de Lagrange en los puntos $x_{m_1}, \cdots, x_{m_k}$.

A continuación damos una fórmula recursiva para calcular un polinomio interpolador de $n + 1$ puntos, dados ciertos otros dos polinomios de $n$ puntos.

\begin{lema}
\label{lema:interp}
Sean $(x_0, y_0), \cdots, (x_n, y_n) \in \mathbb{R}^2$ con $x_i \neq x_j$ si $i \neq j$. Entonces, si $i \neq j$,

\[P_{0, \cdots, n}(x) = \frac{(x - x_j) P_{0, \cdots, j - 1, j + 1, \cdots, n}(x) - (x - x_i) P_{0, \cdots, i - 1, i + 1, \cdots, n}(x)}{x_i - x_j}\]

\begin{proof}
Es claro que $P_{0, \cdots, n}$ es un polinomio de grado menor o igual que $n$. Veamos que interpola los $n + 1$ puntos.

Si $k \neq i, j$, entonces

\begin{align*}
P_{0, \cdots, n}(x_k) &= \frac{(x_k - x_j) P_{0, \cdots, j - 1, j + 1, \cdots, n}(x_k) - (x_k - x_i) P_{0, \cdots, i - 1, i + 1, \cdots, n}(x_k)}{x_i - x_j}\\
&= \frac{(x_k - x_j) y_k - (x_k - x_i) y_k}{x_i - x_j}\\
&= \frac{(x_i - x_j) y_k}{x_i - x_j}\\
&= y_k
\end{align*}

En $x_i$ el polinomio vale

\begin{align*}
P_{0, \cdots, n}(x_i) &= \frac{(x_i - x_j) P_{0, \cdots, j - 1, j + 1, \cdots, n}(x_i) - (x_i - x_i) P_{0, \cdots, i - 1, i + 1, \cdots, n}(x_i)}{x_i - x_j}\\
&= \frac{(x_i - x_j) y_i}{x_i - x_j}\\
&= y_i
\end{align*}

Análogamente, $P_{0, \cdots, n}(x_j) = y_j$.

\end{proof}
\end{lema}

\subsection{Diferencias divididas}

Planteamos un nuevo problema. Supongamos que se tiene una interpolación de $n$ puntos y se la desea extender con un punto nuevo. La forma del polinomio interpolador vista antes no nos provee una forma de aprovechar el polinomio ya calculado, y nos obliga a computar un polinomio interpolador para $n + 1$ puntos desde cero. Queremos encontrar una forma para el polinomio interpolador que permita agregar secuencialmente nuevos puntos con un menor costo.

\begin{defi}
Sean $x_0, \cdots, x_n$ distintos y $f$ una función real. Se define la diferencia dividida de orden 0 de $f$ respecto de $x_i$ como

\[f[x_i] = f(x_i)\]

Para $k > 0$ se define la diferencia dividida de orden $k$ de $f$ respecto de $x_i, \cdots, x_{i + k}$ como

\[f[x_i, \cdots, x_{i + k}] = \frac{f[x_{i + 1}, \cdots, x_{i + k}] - f[x_i, \cdots, x_{i + k - 1}]}{x_{i + k} - x_i}\]
\end{defi}

La importancia de las diferencias divididas radica en el siguiente resultado:

\begin{propo}
\[P_{0, \cdots, n}(x) = a_0 + a_1 (x - x_0) + \cdots + a_n (x - x_0)\cdots (x - x_{n - 1})\]

con $a_k = f[x_0, \cdots, a_k]$.
\begin{proof}
Hacemos inducción en la cantidad $N$ de puntos interpolados. Si $N = 1$ no hay nada que ver, pues $P_0 = f(x_0) = f[x_0]$. Si $N = 2$ entonces

\[f[x_0] + f[x_0, x_1] (x - x_0) = f(x_0) + \frac{f(x_1) - f(x_0)}{x_1 - x_0} (x - x_0)\]

y es fácil verificar que este polinomio interpola $x_0$ y $x_1$, y como tiene grado menor o igual a 1, entonces, por la unicidad del polinomio interpolador, es exactamente $P_{0, 1}$. 

Sea $N > 2$ y $n = N - 1$. Supongamos que $P$ es un polinomio que interpola los $N$ puntos $x_0, \cdots, x_n$, y que es de la forma

\[P(x) = P_{0, \cdots, n - 1}(x) + \alpha (x - x_0) \cdots (x - x_{n - 1})\]

con $\alpha \in \mathbb{R}$ cierta constante fija. Entonces $P(x_i) = P_{0, \cdots, n - 1}(x_i) = f(x_i)$ para todo $i = 0, \cdots, n - 1$, y $P(x_n) = P_{0, \cdots, n - 1}(x_n) + \alpha \prod_{i = 0}^{n - 1}(x_n - x_i)$. Como $P(x_n) = f(x_n)$ por definición, entonces 

\[\alpha = \frac{f(x_n) - P_{0, \cdots, n - 1}(x_n)}{\prod_{i = 0}^{n - 1}(x_n - x_i)}\]

Se puede ver que bajo esta elección de $\alpha$, el polinomio $P_{0, \cdots, n - 1}(x) + \alpha \prod_{i = 0}^{n - 1}(x - x_i)$ interpola todos los puntos $x_0, \cdots, x_n$. Nuevamente por la unicidad del polinomio interpolador resulta que 

\[P_{0, \cdots, n}(x) = P_{0, \cdots, n - 1}(x) + \alpha \prod_{i = 0}^{n - 1}(x - x_i)\]

Por hipótesis inductiva

\[P_{0, \cdots, n - 1}(x) = a_0 + a_1 (x - x_0) + \cdots + a_{n - 1} (x - x_0) \cdots (x - x_{n - 2})\]

Luego, basta probar que $\alpha = a_n$. Notemos que $\alpha$ es el coeficiente del monomio $x^n$ en $P_{0, \cdots, n}(x)$.

Por el Lema \ref{lema:interp},

\[P_{0, \cdots, n}(x) = \frac{(x - x_0)P_{1, \cdots, n}(x) - (x - x_n)P_{0, \cdots, n - 1}(x)}{x_n - x_0}\]

De aquí es fácil ver que el coeficiente de $x^n$ está dado por los coeficientes de los monomios $x^{n - 1}$ en $P_{1, \cdots, n}(x)$ y $P_{0, \cdots, n - 1}(x)$. Por hipótesis inductiva, el coeficiente en $P_{1, \cdots, n}(x)$ es $f[x_1, \cdots, x_n]$ y en $P_{0, \cdots, n - 1}(x)$ es $f[x_0, \cdots, x_n]$. Entonces 

\[\alpha = \frac{f[x_1, \cdots, x_n] - f[x_0, \cdots, x_n]}{x_n - x_0} = f[x_0, \cdots, x_n] = a_n\]
\end{proof}
\end{propo}

\subsection{Interpolación segmentada}

Los polinomios tienen una gran desventaja como interpoladores y es que cuanto mayor es su grado, más oscilan. Un procedimiento alternativo consiste en construir, dados $x_0 < \cdots < x_n$, un polinomio interpolador entre cada par consecutivo de puntos $x_i$ y $x_{i + 1}$, y a partir de estos construir una función que interpole todos los puntos. Es decir, construimos $S_0, \cdots, S_{n - 1}$ polinomios, tal que $S_i$ interpola $x_i$ y $x_{i + 1}$, y definimos

\[
S(x) = 
\left\{
	\begin{array}{ll}
		S_0(x)  & \mbox{si } x \in [x_{0}, x_1] \\
		\hspace{0.3cm}\vdots \\		
		S_{n - 1}(x) & \mbox{si } x \in [x_{n - 1}, x_n]
	\end{array}
\right.
\]

\subsubsection{Lineal}
Consiste en interpolar cada par de puntos con un polinomio de grado 1. En otras palabras, $S_i$ es el polinomio interpolador de Lagrange en los puntos $x_i$ y $x_{i - 1}$. La desventaja de este tipo de interpolación es que en los extremos de los subintervalos no hay garantía de que $S$ sea derivable (geométricamente la curva no es suave).

\subsubsection{Cuadrática}

Utilizamos $S_i(x) = a_i + b_i x + c_i x^2$ para ciertas constantes $a_i, b_i$ y $c_i$. Si $f$ es la función que estamos aproximando, estas constantes deben ser ajustadas de modo tal que

\begin{enumerate}
\item $S(x_i) = f(x_i)$ para todo $i = 0, \cdots, n$.
\item $S_{i + 1}(x_{i + 1}) = S_i(x_{i + 1})$, para todo $i = 0, \cdots, n - 2$.
\item $S_{i + 1}'(x_{i + 1}) = S_i'(x_{i + 1})$, para todo $i = 0, \cdots, n - 2$.
\end{enumerate}

Las condiciones 1 y 2 aseguran que $S$ esté bien definido y sea contínuo en $[x_0, x_n]$. La condición 3 asegura que sea derivable en $(x_0, x_n)$. Una curva diferenciable definida por partes mediante polinomios se denomina \textit{spline}.

Notemos que en total son $3n - 1$ ecuaciones y $3n$ incógnitas, dejando un grado de libertad.

El inconveniente de esta interpolación es que muchas veces se desea fijar condiciones para $S'(x)$ en los extremos $x_0$ y $x_n$, y sin embargo no hay constantes suficientes para ello. Los polinomios cúbicos solucionan este problema.

\subsubsection{Cúbica}
Un spline cúbico para $f$ es una función $S$ que cumple las siguientes condiciones

\begin{enumerate}
\item $
S(x) = 
\left\{
	\begin{array}{ll}
		S_0(x)  & \mbox{si } x \in [x_{0}, x_1] \\
		\hspace{0.3cm}\vdots \\		
		S_{n - 1}(x) & \mbox{si } x \in [x_{n - 1}, x_n]
	\end{array}
\right.
$, con $S_i$ un polinomio cúbico.

\item $S(x_i) = f(x_i)$ para todo $i = 0, \cdots, n$.
\item $S_{i + 1}(x_{i + 1}) = S_i(x_{i + 1})$ para todo $i = 0, \cdots, n - 2$.
\item $S_{i + 1}'(x_{i + 1}) = S_i'(x_{i + 1})$ para todo $i = 0, \cdots, n - 2$.
\item $S_{i + 1}''(x_{i + 1}) = S_i''(x_{i + 1})$ para todo $i = 0, \cdots, n - 2$.
\item Se satisface una de las siguientes condiciones frontera:
\begin{itemize}
\item $S''(x_0) = S''(x_n) = 0$ (spline libre o natural).
\item $S'(x_0) = f'(x_0)$ y $S'(x_n) = f'(x_n)$ (spline sujeto).
\end{itemize} 
\end{enumerate}

Las condiciones 2 y 3 aseguran que $S$ esté bien definido y sea contínuo en $[x_0, x_n]$. Las condiciones 4 y 5 aseguran que $S$ sea dos veces derivable y, más aún, como es unión de polinomios, entonces las derivadas son contínuas. Las condiciones de frontera sujeta se utilizan cuando tengo esa información sobre la derivada de la función, y dan lugar a aproximaciones más exactas.

Estudiemos las ecuaciones que determinan las condiciones anteriores. Por conveniencia, vamos a considerar polinomios cúbicos de la forma

\[S_i(x) = a_i + b_i (x - x_i) + c_i (x - x_i)^2 + d_i (x - x_i)^3\]

La condición 2 equivale a $S_i(x_i) = f(x_i)$ y $S_{n - 1}(x_n) = f(x_n)$. Como $S_i(x_i) = a_i$ entonces tenemos

\[a_i = f(x_i) \hspace{0.3cm}\text{para todo } i = 0, \cdots, n - 1\]
\[a_{n - 1} + b_{n - 1} (x_n - x_{n - 1}) + c_{n - 1} (x_n - x_{n - 1})^2 + d_{n - 1} (x_n - x_{n - 1})^3 = f(x_n)\]

La condición 3 equivale a $S_i(x_{i + 1}) = f(x_{i + 1})$. Entonces

\[a_i + b_i (x_{i + 1} - x_i) + c_i (x_{i + 1} - x_i)^2 + d_i (x_{i + 1} - x_i)^3 = a_{i + 1} \hspace{0.3cm}\text{para todo } i = 0, \cdots, n - 2\]

Observar que $S_i'(x) = b_i + 2c_i(x - x_i) + 3 d_i (x - x_i)^2$. Como $S_{i + 1}'(x_{i + 1}) = b_{i + 1}$ entonces la condición 4 equivale a 

\[b_i + 2c_i(x_{i + 1} - x_i) + 3 d_i (x_{i + 1} - x_i)^2 = b_{i + 1} \hspace{0.3cm}\text{para todo } i = 0, \cdots, n - 2\]

Observar que $S_i''(x) = 2c_i + 6d_i(x - x_i)$. Como $S_{i + 1}''(x_{i + 1}) = 2c_{i + 1}$ entonces la condición 5 equivale a

\[2c_i + 6d_i(x_{i + 1} - x_i) = 2c_{i + 1} \hspace{0.3cm}\text{para todo } i = 0, \cdots, n - 2\]

Finalmente, si el spline es libre, la condición 6 equivale a

\[2c_0 = 0\]
\[2c_{n - 1} + 6d_{n - 1}(x_n - x_{n - 1}) = 0\]

Definamos $a_n = f(x_n)$, $c_n = 0$ y $h_i = x_{i + 1} - x_i$. Entonces las ecuaciones son

\begin{enumerate}
\item $a_i = f(x_i)$ para todo $0 \leq i \leq n$.
\item $a_i + b_ih_{i} + c_i h_{i}^2 + d_i h_{i}^3 = a_{i + 1}$ para todo $ 0 \leq i \leq n - 1$.
\item $b_i + 2c_i h_{i} + 3d_i h_{i} ^2 = b_{i + 1}$ para todo $0 \leq i \leq n - 2$.
\item $2c_i + 6d_i h_{i} = 2c_{i + 1}$ para todo $ 0 \leq i \leq n - 1$.
\item $c_0 = 0$.
\end{enumerate} 

De la ecuación 4 despejamos

\[d_i = \frac{c_{i + 1} - c_i}{3 h_{i}}\]

Sustituyendo esto último y la ecuación 1 en la ecuación 2 y despejando

\begin{align*}
b_i &= \frac{1}{h_{i}}\left(f(x_{i + 1}) - f(x_i) - c_i h_{i}^2 - d_i h_{i}^3)\right)\\
	&= \frac{1}{h_{i}}\left(f(x_{i + 1}) - f(x_i) - c_i h_{i}^2 - \frac{(c_{i + 1} - c_i) h_{i}^3}{3 h_{i}}\right)\\
	&= \frac{f(x_{i + 1}) - f(x_i)}{h_{i}} - c_i h_{i} - \frac{1}{3} (c_{i + 1} - c_i) h_{i}\\
	&= \frac{f(x_{i + 1}) - f(x_i)}{h_{i}} - \frac{h_{i}}{3}\left(2c_i + c_{i + 1}\right)
\end{align*}

Sustituyendo en la ecuación 3 y despejando

\begin{align*}
0 & = b_i - b_{i + 1} + 2c_ih_{i} + 3d_i h_{i}^2\\
	& = \frac{f(x_{i + 1}) - f(x_i)}{h_{i}} - \frac{h_{i}}{3}\left(2c_i + c_{i + 1}\right) -
		\frac{f(x_{i + 2}) - f(x_{i + 1})}{h_{i + 1}} + \frac{h_{i + 1}}{3}\left(2c_{i + 1} + c_{i + 2}\right)\\
		& +2c_ih_{i} + 3\frac{c_{i + 1} - c_i}{3 h_{i}} h_{i}^2\\
	& = \left[\frac{f(x_{i + 1}) - f(x_i)}{h_{i}} - \frac{f(x_{i + 2}) - f(x_{i + 1})}{h_{i + 1}}\right]
		 - \frac{2}{3} h_{i} c_i - \frac{1}{3} h_{i} c_{i + 1} + \frac{2}{3} h_{i + 1} c_{i + 1} + \frac{1}{3} h_{i + 1} c_{i + 2}\\
		& + 2h_{i} c_i + h_{i} c_{i + 1} - h_{i}c_i\\
	& = \left[\frac{f(x_{i + 1}) - f(x_i)}{h_{i}} - \frac{f(x_{i + 2}) - f(x_{i + 1})}{h_{i + 1}}\right]
		 + \frac{1}{3}h_i c_i + \frac{2}{3} (h_i + h_{i + 1}) c_{i + 1} + \frac{1}{3}h_{i + 1} c_{i + 2}\\
\end{align*}

Equivalentemente

\[h_i c_i + 2 (h_i + h_{i + 1}) c_{i + 1} + h_{i + 1} c_{i + 2} = 
3 \left[\frac{f(x_{i + 2}) - f(x_{i + 1})}{h_{i + 1}} - \frac{f(x_{i + 1}) - f(x_i)}{h_{i}}\right]
\]

Esta ecuación, que vale para $i = 0, \cdots, n - 2$, contiene toda la información de las demás (pues la obtuvimos a través de sustituciones sucesivas). Juntando estas $n - 1$ ecuaciones con $c_0 = 0$ y $c_n = 0$ tenemos un sistema de $n + 1$ ecuaciones y $n + 1$ incógnitas

\[
\begin{pmatrix}
1 		& 0 				& 0 				& 0 			& 			&  			& 							&			\\
h_0 		& 2(h_0 + h_1) 	& h_1 			& 0 			&		 	& 			&							& 			\\
0 		& h_1			& 2(h_1 + h_2)	& h_2 		& 		 	&  			&							& 			\\
		&				&				& 			& \ddots		& 			&							& 			\\
		&				&				& 			& 			& h_{n - 2} & 2(h_{n - 2} + h_{n - 1}) 	& h_{n - 1}\\
		&				&				&			&			&	0		&			0				& 1			\\
\end{pmatrix}\]

La matriz del sistema es estrictamente diagonal dominante (pues $h_i = x_{i + 1} - x_i > 0$), por lo tanto es inversible. Luego, la solución es única, i.e., existe un único spline cúbico natural para $x_0, \cdots, x_n$. Más aún, como la matriz es tridiagonal puede ser almacenada y operada eficientemente. En particular, el costo de la eliminación gaussiana sobre esta matriz es $\mathcal{O}(n)$.

Procediendo en forma análoga se puede demostrar que si el spline cúbico es sujeto también es único. Más aún, también se obtiene un sistema tridiagonal estrictamente diagonal dominante.